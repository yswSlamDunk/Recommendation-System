{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ae9745a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.sparse as sp\n",
    "import argparse\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7bddf1",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "07232474",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Data/Movie_Lens_100k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2b6089e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(path, 'ua.base'), sep = '\\t', names = ['user_id', 'movie_id', 'rating', 'timestamp'])\n",
    "test_df = pd.read_csv(os.path.join(path, 'ua.test'), sep = '\\t', names = ['user_id', 'movie_id', 'rating', 'timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "197e945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_only_movie = list(set(test_df['movie_id'].unique().flatten()) - set(train_df['movie_id'].unique().flatten()))\n",
    "test_df = test_df[~test_df['movie_id'].isin(test_only_movie)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6534ef2f",
   "metadata": {},
   "source": [
    "### Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f3636218",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_le = LabelEncoder()\n",
    "movie_le = LabelEncoder()\n",
    "\n",
    "user_le.fit(train_df['user_id'])\n",
    "movie_le.fit(train_df['movie_id'])\n",
    "\n",
    "train_df['user_id'] = user_le.transform(train_df['user_id'])\n",
    "train_df['movie_id'] = movie_le.transform(train_df['movie_id'])\n",
    "\n",
    "test_df['user_id'] = user_le.transform(test_df['user_id'])\n",
    "test_df['movie_id'] = movie_le.transform(test_df['movie_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "032d65cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>874965758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>876893171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>878542960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>876893119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>889751712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating  timestamp\n",
       "0        0         0       5  874965758\n",
       "1        0         1       3  876893171\n",
       "2        0         2       4  878542960\n",
       "3        0         3       3  876893119\n",
       "4        0         4       3  889751712"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b201b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9afcdf06",
   "metadata": {},
   "source": [
    "# NGCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "318745f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGCF(nn.Module):\n",
    "    def __init__(self, n_user, n_item, norm_adj, args):\n",
    "        super(NGCF, self).__init__()\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.device = args.device\n",
    "        self.emb_size = args.embed_size\n",
    "        self.batch_size = args.batch_size\n",
    "        \n",
    "        self.node_dropout = args.node_dropout\n",
    "        self.mess_dropout = args.mess_dropout\n",
    "        self.batch_size = args.batch_size\n",
    "        \n",
    "        self.norm_adj = norm_adj\n",
    "        \n",
    "        self.layers = eval(args.layer_size)\n",
    "        self.decay = eval(args.regs)[0]\n",
    "        \n",
    "        # Init the weight of user-item.\n",
    "        self.embedding_dict, self.weight_dict = self.init_weight()\n",
    "        \n",
    "        # Get sparse adj\n",
    "        self.sparse_norm_adj = self._convert_sp_mat_to_sp_tensor(self.norm_adj).to(self.device)\n",
    "        \n",
    "    def init_weight(self):\n",
    "        #xavier init\n",
    "        initializer = nn.init.xavier_uniform_\n",
    "        \n",
    "        embedding_dict = nn.ParameterDict({\n",
    "            'user_emb' : nn.Parameter(initializer(torch.empty(self.n_user, self.emb_size))),\n",
    "            'item_emb' : nn.Parameter(initializer(torch.empty(self.n_item, self.emb_size)))\n",
    "        })\n",
    "        \n",
    "        weight_dict = nn.ParameterDict()\n",
    "        layers = [self.emb_size] + self.layers\n",
    "        for k in range(len(self.layers)):\n",
    "            weight_dict.update({'W_gu_%d'%k: nn.Parameter(initializer(torch.empty(layers[k], layers[k + 1])))})\n",
    "            weight_dict.update({'b_gu_%d'%k: nn.Parameter(initializer(torch.empty(1, layers[k + 1])))})\n",
    "            weight_dict.update({'W_bi_%d'%k: nn.Parameter(initializer(torch.empty(layers[k], layers[k + 1])))})\n",
    "            weight_dict.update({'b_bi_%d'%k: nn.Parameter(initializer(torch.empty(1, layers[k + 1])))})\n",
    "            \n",
    "        return embedding_dict, weight_dict\n",
    "    \n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        coo = X.tocoo()\n",
    "        i = torch.LongTensor([coo.row, coo.col])\n",
    "        v = torch.from_numpy(coo.data).float()\n",
    "        return torch.sparse.FloatTensor(i, v, coo.shape)\n",
    "    \n",
    "    def sparse_dropout(self, x, rate, noise_shape):       \n",
    "        rate = rate[0]\n",
    "        \n",
    "        random_tensor = 1 - rate\n",
    "        random_tensor += torch.rand(noise_shape).to(x.device)\n",
    "        dropout_mask = torch.floor(random_tensor).type(torch.bool)\n",
    "        i = x._indices()\n",
    "        v = x._values()\n",
    "        \n",
    "        i = i[:, dropout_mask]\n",
    "        v = v[dropout_mask]\n",
    "        \n",
    "        out = torch.sparse.FloatTensor(i, v, x.shape).to(x.device)\n",
    "        return out * (1. / (1 - rate))\n",
    "    \n",
    "    def loss(self, users_embedding, items_embedding, labels):\n",
    "        scores = torch.sum(torch.mul(users_embedding, items_embedding), axis = 1)           \n",
    "        loss_value = nn.MSELoss()(scores, labels)\n",
    "        return loss_value\n",
    "        \n",
    "    def rating(self, user_embedding, item_embedding):\n",
    "        return torch.matmul(user_embedding, item_embedding.t()).item()\n",
    "    \n",
    "    def forward(self, users, items, drop_flag = True):\n",
    "        A_hat = self.sparse_dropout(self.sparse_norm_adj, \n",
    "                                    self.node_dropout, \n",
    "                                    self.sparse_norm_adj._nnz()) if drop_flag else self.sparse_norm_adj\n",
    "        \n",
    "        ego_embeddings = torch.cat([self.embedding_dict['user_emb'], \n",
    "                                    self.embedding_dict['item_emb']], 0)\n",
    "        \n",
    "        all_embeddings = [ego_embeddings]\n",
    "        \n",
    "        for k in range(len(self.layers)):\n",
    "            side_embeddings = torch.sparse.mm(A_hat, ego_embeddings)\n",
    "            \n",
    "            sum_embeddings = torch.matmul(side_embeddings, self.weight_dict['W_gu_%d' %k]) + self.weight_dict['b_gu_%d' %k]\n",
    "            bi_embeddings = torch.mul(ego_embeddings, side_embeddings)\n",
    "            bi_embeddings = torch.matmul(bi_embeddings, self.weight_dict['W_bi_%d' %k]) + self.weight_dict['b_bi_%d' %k]\n",
    "            \n",
    "            ego_embeddings = nn.LeakyReLU(negative_slope = 0.2)(sum_embeddings + bi_embeddings)\n",
    "            \n",
    "            ego_embeddings = nn.Dropout(self.mess_dropout[k])(ego_embeddings)\n",
    "            \n",
    "            norm_embeddings = F.normalize(ego_embeddings, p = 2, dim = 1)\n",
    "            all_embeddings += [norm_embeddings]\n",
    "            \n",
    "        all_embeddings = torch.cat(all_embeddings, 1)\n",
    "        u_g_embeddings = all_embeddings[:self.n_user, :]\n",
    "        i_g_embeddings = all_embeddings[self.n_user:, :]\n",
    "        \n",
    "        u_g_embeddings = u_g_embeddings[users, :]\n",
    "        i_g_embeddings = i_g_embeddings[items, :]\n",
    "        \n",
    "        return u_g_embeddings, i_g_embeddings                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73da2106",
   "metadata": {},
   "source": [
    "### Graph class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "acd2dd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph(object):\n",
    "    def __init__(self, train_df, test_df, user_le, item_le, batch_size):\n",
    "#         self.path = path\n",
    "        self.batch_size = batch_size\n",
    "       \n",
    "        self.user_le = user_le\n",
    "        self.item_le = item_le\n",
    "        \n",
    "        self.n_users = len(train_df['user_id'].unique())\n",
    "        self.n_items = len(train_df['movie_id'].unique())\n",
    "        \n",
    "        self.n_train = len(train_df)\n",
    "        self.n_test = len(test_df)\n",
    "        \n",
    "        self.R = sp.dok_matrix((self.n_users, self.n_items), dtype = np.float32)\n",
    "        \n",
    "        self.train_items, self.test_set = {}, {}\n",
    "        self.exist_users = list(train_df['user_id'].unique())\n",
    "        \n",
    "        for i, row in train_df.iterrows():\n",
    "            \n",
    "            # 여기에 rating 값을 넣어야 하는지 생갈할 필요가 있다.\n",
    "            self.R[row['user_id'], row['movie_id']] = 1\n",
    "        \n",
    "        for i, row in enumerate(train_df.groupby(['user_id'])['movie_id'].unique()):\n",
    "            self.train_items[i] = row\n",
    "            \n",
    "        for i, row in enumerate(test_df.groupby(['user_id'])['movie_id'].unique()):\n",
    "            self.test_set[i] = row\n",
    "        \n",
    "    def get_adj_mat(self):\n",
    "        adj_mat, norm_adj_mat, mean_adj_mat = self.create_adj_mat()\n",
    "        return adj_mat, norm_adj_mat, mean_adj_mat\n",
    "    \n",
    "    def create_adj_mat(self):\n",
    "        adj_mat = sp.dok_matrix((self.n_users + self.n_items, self.n_users + self.n_items), dtype = np.float32)\n",
    "        adj_mat = adj_mat.tolil()\n",
    "        R = self.R.tolil()\n",
    "        \n",
    "        adj_mat[:self.n_users, self.n_users:] = R\n",
    "        adj_mat[self.n_users:, :self.n_users] = R.T\n",
    "        adj_mat = adj_mat.todok()\n",
    "        \n",
    "        def mean_adj_single(adj):\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "            \n",
    "            d_inv = np.power(rowsum, -1).flatten()\n",
    "            d_inv[np.isinf(d_inv)] = 0.\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "            \n",
    "            norm_adj = d_mat_inv.dot(adj)\n",
    "            return norm_adj.tocoo()\n",
    "        \n",
    "        def normalized_adj_single(adj):\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "            \n",
    "            d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "            d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "            d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "            \n",
    "            bi_lap = d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt)\n",
    "            return bi_lap.tocoo()\n",
    "        \n",
    "        def check_adj_if_equal(adj):\n",
    "            dense_A = np.array(adj.todense())\n",
    "            degree = np.sum(dense_A, axis = 1, keepdims = False)\n",
    "            \n",
    "            temp = np.dot(np.diag(np.power(degree, -1)), dense_A)\n",
    "            return temp\n",
    "        \n",
    "        norm_adj_mat = mean_adj_single(adj_mat + sp.eye(adj_mat.shape[0]))\n",
    "        mean_adj_mat = mean_adj_single(adj_mat)\n",
    "        return adj_mat.tocsr(), norm_adj_mat.tocsr(), mean_adj_mat.tocsr()\n",
    "    \n",
    "    \n",
    "    def get_num_users_items(self):\n",
    "        return self.n_users, self.n_items        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "dace29d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.df = df\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        user = torch.tensor(row[0], dtype = torch.long)\n",
    "        item = torch.tensor(row[1], dtype = torch.long)\n",
    "        label = torch.tensor(row[2], dtype = torch.float)\n",
    "        \n",
    "        return user, item, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc0e49",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53744342",
   "metadata": {},
   "source": [
    "### argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "0a8cbea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--report'], dest='report', nargs=None, const=None, default=0, type=<class 'int'>, choices=None, help='0: Disable performance report w.r.t. sparsity levels, 1: Show performance report w.r.t. sparsity levels', metavar=None)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Run NGCF.\")\n",
    "parser.add_argument('--weights_path', nargs='?', default='model/',\n",
    "                    help='Store model path.')\n",
    "parser.add_argument('--data_path', nargs='?', default='../Data/',\n",
    "                    help='Input data path.')\n",
    "parser.add_argument('--proj_path', nargs='?', default='',\n",
    "                    help='Project path.')\n",
    "\n",
    "parser.add_argument('--dataset', nargs='?', default='gowalla',\n",
    "                    help='Choose a dataset from {gowalla, yelp2018, amazon-book}')\n",
    "parser.add_argument('--pretrain', type=int, default=0,\n",
    "                    help='0: No pretrain, -1: Pretrain with the learned embeddings, 1:Pretrain with stored models.')\n",
    "parser.add_argument('--verbose', type=int, default=1,\n",
    "                    help='Interval of evaluation.')\n",
    "parser.add_argument('--epoch', type=int, default=100,\n",
    "                    help='Number of epoch.')\n",
    "\n",
    "parser.add_argument('--embed_size', type=int, default=64,\n",
    "                    help='Embedding size.')\n",
    "parser.add_argument('--layer_size', nargs='?', default='[64,64,64]',\n",
    "                    help='Output sizes of every layer')\n",
    "parser.add_argument('--batch_size', type=int, default=1024,\n",
    "                    help='Batch size.')\n",
    "\n",
    "parser.add_argument('--regs', nargs='?', default='[1e-5]',\n",
    "                    help='Regularizations.')\n",
    "parser.add_argument('--lr', type=float, default=0.001,\n",
    "                    help='Learning rate.')\n",
    "\n",
    "parser.add_argument('--model_type', nargs='?', default='ngcf',\n",
    "                    help='Specify the name of model (ngcf).')\n",
    "parser.add_argument('--adj_type', nargs='?', default='norm',\n",
    "                    help='Specify the type of the adjacency (laplacian) matrix from {plain, norm, mean}.')\n",
    "\n",
    "parser.add_argument('--gpu_id', type=int, default=6)\n",
    "\n",
    "parser.add_argument('--node_dropout_flag', type=int, default=1,\n",
    "                    help='0: Disable node dropout, 1: Activate node dropout')\n",
    "parser.add_argument('--node_dropout', nargs='?', default='[0.1]',\n",
    "                    help='Keep probability w.r.t. node dropout (i.e., 1-dropout_ratio) for each deep layer. 1: no dropout.')\n",
    "parser.add_argument('--mess_dropout', nargs='?', default='[0.1,0.1,0.1]',\n",
    "                    help='Keep probability w.r.t. message dropout (i.e., 1-dropout_ratio) for each deep layer. 1: no dropout.')\n",
    "\n",
    "parser.add_argument('--Ks', nargs='?', default='[20, 40, 60, 80, 100]',\n",
    "                    help='Output sizes of every layer')\n",
    "\n",
    "parser.add_argument('--save_flag', type=int, default=0,\n",
    "                    help='0: Disable model saver, 1: Activate model saver')\n",
    "\n",
    "parser.add_argument('--test_flag', nargs='?', default='part',\n",
    "                    help='Specify the test type from {part, full}, indicating whether the reference is done in mini-batch')\n",
    "\n",
    "parser.add_argument('--report', type=int, default=0,\n",
    "                    help='0: Disable performance report w.r.t. sparsity levels, 1: Show performance report w.r.t. sparsity levels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "a44d01f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, train_loss : 1.9681, test_loss : 2.048\n",
      "epoch : 1, train_loss : 1.8566, test_loss : 1.863\n",
      "epoch : 2, train_loss : 1.3797, test_loss : 1.3629\n",
      "epoch : 3, train_loss : 0.9798, test_loss : 1.0979\n",
      "epoch : 4, train_loss : 0.8807, test_loss : 1.0355\n",
      "epoch : 5, train_loss : 0.8261, test_loss : 0.9748\n",
      "epoch : 6, train_loss : 0.7727, test_loss : 0.9554\n",
      "epoch : 7, train_loss : 0.731, test_loss : 0.9175\n",
      "epoch : 8, train_loss : 0.6983, test_loss : 0.918\n",
      "epoch : 9, train_loss : 0.6659, test_loss : 0.9037\n",
      "epoch : 10, train_loss : 0.6331, test_loss : 0.8987\n",
      "epoch : 11, train_loss : 0.6022, test_loss : 0.8863\n",
      "epoch : 12, train_loss : 0.5739, test_loss : 0.8918\n",
      "epoch : 13, train_loss : 0.5458, test_loss : 0.8927\n",
      "epoch : 14, train_loss : 0.5177, test_loss : 0.8775\n",
      "epoch : 15, train_loss : 0.4927, test_loss : 0.8921\n",
      "epoch : 16, train_loss : 0.4681, test_loss : 0.8939\n",
      "epoch : 17, train_loss : 0.4439, test_loss : 0.8943\n",
      "epoch : 18, train_loss : 0.4229, test_loss : 0.8949\n",
      "epoch : 19, train_loss : 0.4002, test_loss : 0.9001\n",
      "epoch : 20, train_loss : 0.3807, test_loss : 0.9202\n",
      "epoch : 21, train_loss : 0.3617, test_loss : 0.9167\n",
      "epoch : 22, train_loss : 0.3444, test_loss : 0.9372\n",
      "epoch : 23, train_loss : 0.3281, test_loss : 0.9275\n",
      "epoch : 24, train_loss : 0.312, test_loss : 0.9389\n",
      "epoch : 25, train_loss : 0.298, test_loss : 0.9535\n",
      "epoch : 26, train_loss : 0.284, test_loss : 0.9474\n",
      "epoch : 27, train_loss : 0.2715, test_loss : 0.9367\n",
      "epoch : 28, train_loss : 0.26, test_loss : 0.9604\n",
      "epoch : 29, train_loss : 0.2483, test_loss : 0.968\n",
      "epoch : 30, train_loss : 0.2378, test_loss : 0.9811\n",
      "epoch : 31, train_loss : 0.2277, test_loss : 0.9938\n",
      "epoch : 32, train_loss : 0.2189, test_loss : 1.0032\n",
      "epoch : 33, train_loss : 0.2103, test_loss : 1.0031\n",
      "epoch : 34, train_loss : 0.2023, test_loss : 1.0061\n",
      "epoch : 35, train_loss : 0.1942, test_loss : 1.0095\n",
      "epoch : 36, train_loss : 0.187, test_loss : 1.0273\n",
      "epoch : 37, train_loss : 0.1811, test_loss : 1.0346\n",
      "epoch : 38, train_loss : 0.1739, test_loss : 1.0386\n",
      "epoch : 39, train_loss : 0.1684, test_loss : 1.0573\n",
      "epoch : 40, train_loss : 0.1627, test_loss : 1.0418\n",
      "epoch : 41, train_loss : 0.1582, test_loss : 1.0455\n",
      "epoch : 42, train_loss : 0.153, test_loss : 1.0584\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7130/1408875767.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0musers_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gnn_rec/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gnn_rec/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gnn_rec/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gnn_rec/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7130/3241881007.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0muser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gnn_rec/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    943\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0mcheck_deprecated_indexers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mEllipsis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gnn_rec/lib/python3.9/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mapply_if_callable\u001b[0;34m(maybe_callable, obj, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \"\"\"\n\u001b[0;32m--> 357\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = parser.parse_args('')\n",
    "args.device = torch.device('cpu')\n",
    "\n",
    "data_generator = Graph(train_df, test_df, user_le, movie_le, args.batch_size)\n",
    "\n",
    "plain_adj, norm_adj, mean_adj = data_generator.get_adj_mat()\n",
    "\n",
    "args.node_dropout = eval(args.node_dropout)\n",
    "args.mess_dropout = eval(args.mess_dropout)\n",
    "\n",
    "\n",
    "model = NGCF(data_generator.n_users,\n",
    "            data_generator.n_items,\n",
    "            norm_adj,\n",
    "            args).to(args.device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = args.lr)\n",
    "\n",
    "train_dataset = CustomDataset(train_df)\n",
    "test_dataset = CustomDataset(test_df)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = args.batch_size, shuffle = True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = args.batch_size, shuffle = True)\n",
    "\n",
    "train_loss_loger, test_loss_loger = [], []\n",
    "\n",
    "for epoch in range(args.epoch):\n",
    "    train_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for users, items, labels in train_dataloader:\n",
    "        users_embedding, items_embedding = model(users, items)\n",
    "                \n",
    "        batch_loss = model.loss(users_embedding, items_embedding, labels)\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += batch_loss.item()/len(train_dataloader)\n",
    "    \n",
    "    train_loss_loger.append(train_loss)\n",
    "    \n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for users, items, labels in test_dataloader:\n",
    "            users_embedding, items_embedding = model(users, items)\n",
    "            \n",
    "            batch_loss = model.loss(users_embedding, items_embedding, labels)\n",
    "            test_loss += batch_loss.item() / len(test_dataloader)\n",
    "\n",
    "        test_loss_loger.append(test_loss)\n",
    "        \n",
    "    print('epoch : {}, train_loss : {}, test_loss : {}'.format(epoch, round(train_loss, 4), round(test_loss, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201dcb3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_rec",
   "language": "python",
   "name": "gnn_rec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
